{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features based on correlation: ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'radius_se', 'perimeter_se', 'area_se', 'compactness_se', 'concavity_se', 'concave points_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
      "Optimal k for full features: 3\n",
      "Optimal k for reduced features: 3\n",
      "\n",
      "Accuracy with Full Features:\n",
      "KNN: 0.8947368421052632\n",
      "Random Forest: 0.9824561403508771\n",
      "SVC: 0.9649122807017544\n",
      "\n",
      "Accuracy with Reduced Features (Correlation-Based):\n",
      "KNN: 0.9298245614035088\n",
      "Random Forest: 0.9649122807017544\n",
      "SVC: 0.9473684210526315\n",
      "\n",
      "Features selected by RFE: ['radius_se', 'radius_worst', 'texture_worst', 'area_worst', 'concave points_worst']\n",
      "Optimal k for RFE features: 3\n",
      "\n",
      "Accuracy with RFE-Based Features:\n",
      "KNN: 0.9649122807017544\n",
      "Random Forest: 0.9824561403508771\n",
      "SVC: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data_path = r\"C:\\Users\\justino\\Desktop\\Boot Camp\\First Week\\.venv\\data_refined_BC.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Encode the target column ('diagnosis'): M -> malignant, B -> benign\n",
    "le = LabelEncoder()\n",
    "df['diagnosis'] = le.fit_transform(df['diagnosis'])\n",
    "\n",
    "# 2. Feature Selection using Correlation\n",
    "target = 'diagnosis'\n",
    "correlations = {}\n",
    "for feature in df.columns:\n",
    "    if feature != target:\n",
    "        correlations[feature] = df[feature].corr(df[target])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])\n",
    "# Define a threshold for significance (adjust as necessary)\n",
    "threshold = 0.1\n",
    "important_features = corr_df[abs(corr_df['Correlation']) > threshold].index.tolist()\n",
    "print(\"Important features based on correlation:\", important_features)\n",
    "\n",
    "# Prepare datasets\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Full feature set\n",
    "X_full = X.copy()\n",
    "\n",
    "# Reduced feature set based on correlation threshold\n",
    "X_reduced = X[important_features]\n",
    "\n",
    "# Standardize features (especially important for KNN and SVC)\n",
    "scaler_full = StandardScaler()\n",
    "X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "\n",
    "scaler_reduced = StandardScaler()\n",
    "X_reduced_scaled = scaler_reduced.fit_transform(X_reduced)\n",
    "\n",
    "# Splitting the Data\n",
    "# split off 80% for training and 20% for temporary testing/validation\n",
    "X_train_full, X_temp_full, y_train, y_temp = train_test_split(\n",
    "    X_full_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_reduced, X_temp_reduced, _, _ = train_test_split(\n",
    "    X_reduced_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# split the temporary set equally into 10% validation and 10% test sets\n",
    "X_val_full, X_test_full, y_val, y_test = train_test_split(\n",
    "    X_temp_full, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "X_val_reduced, X_test_reduced, _, _ = train_test_split(\n",
    "    X_temp_reduced, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Utility function to train and evaluate a model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    return acc, cm\n",
    "\n",
    "# Training Classifiers\n",
    "\n",
    "# --- KNN Classifier with Cross-Validation to choose optimal k ---\n",
    "knn_params = {'n_neighbors': list(range(1, 31))}\n",
    "\n",
    "# Full feature set KNN\n",
    "knn_full_cv = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
    "knn_full_cv.fit(X_train_full, y_train)\n",
    "best_k_full = knn_full_cv.best_params_['n_neighbors']\n",
    "print(\"Optimal k for full features:\", best_k_full)\n",
    "knn_model_full = KNeighborsClassifier(n_neighbors=best_k_full)\n",
    "acc_knn_full, cm_knn_full = evaluate_model(knn_model_full, X_train_full, y_train, X_val_full, y_val)\n",
    "\n",
    "# Reduced feature set KNN\n",
    "knn_reduced_cv = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
    "knn_reduced_cv.fit(X_train_reduced, y_train)\n",
    "best_k_reduced = knn_reduced_cv.best_params_['n_neighbors']\n",
    "print(\"Optimal k for reduced features:\", best_k_reduced)\n",
    "knn_model_reduced = KNeighborsClassifier(n_neighbors=best_k_reduced)\n",
    "acc_knn_reduced, cm_knn_reduced = evaluate_model(knn_model_reduced, X_train_reduced, y_train, X_val_reduced, y_val)\n",
    "\n",
    "# --- Random Forest Classifier ---\n",
    "rf_full = RandomForestClassifier(random_state=42)\n",
    "acc_rf_full, cm_rf_full = evaluate_model(rf_full, X_train_full, y_train, X_val_full, y_val)\n",
    "\n",
    "rf_reduced = RandomForestClassifier(random_state=42)\n",
    "acc_rf_reduced, cm_rf_reduced = evaluate_model(rf_reduced, X_train_reduced, y_train, X_val_reduced, y_val)\n",
    "\n",
    "# --- Support Vector Classifier (SVC) ---\n",
    "svc_full = SVC(random_state=42)\n",
    "acc_svc_full, cm_svc_full = evaluate_model(svc_full, X_train_full, y_train, X_val_full, y_val)\n",
    "\n",
    "svc_reduced = SVC(random_state=42)\n",
    "acc_svc_reduced, cm_svc_reduced = evaluate_model(svc_reduced, X_train_reduced, y_train, X_val_reduced, y_val)\n",
    "\n",
    "# Print accuracy results for full features\n",
    "print(\"\\nAccuracy with Full Features:\")\n",
    "print(\"KNN:\", acc_knn_full)\n",
    "print(\"Random Forest:\", acc_rf_full)\n",
    "print(\"SVC:\", acc_svc_full)\n",
    "\n",
    "# Print accuracy results for reduced features\n",
    "print(\"\\nAccuracy with Reduced Features (Correlation-Based):\")\n",
    "print(\"KNN:\", acc_knn_reduced)\n",
    "print(\"Random Forest:\", acc_rf_reduced)\n",
    "print(\"SVC:\", acc_svc_reduced)\n",
    "\n",
    "\n",
    "\n",
    "# Challenge: Alternative Feature Reduction using RFE\n",
    "# Using Logistic Regression as the estimator for RFE\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# For example, select the top 5 features\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "selector.fit(X_full_scaled, y)\n",
    "selected_features_rfe = X_full.columns[selector.support_].tolist()\n",
    "print(\"\\nFeatures selected by RFE:\", selected_features_rfe)\n",
    "\n",
    "# Build new dataset based on RFE-selected features\n",
    "X_rfe = X[selected_features_rfe]\n",
    "scaler_rfe = StandardScaler()\n",
    "X_rfe_scaled = scaler_rfe.fit_transform(X_rfe)\n",
    "\n",
    "# Split the RFE-based data (using the same proportions)\n",
    "X_train_rfe, X_temp_rfe, y_train, y_temp = train_test_split(\n",
    "    X_rfe_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val_rfe, X_test_rfe, y_val, y_test = train_test_split(\n",
    "    X_temp_rfe, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Train classifiers on RFE-based features\n",
    "\n",
    "# KNN with cross-validation\n",
    "knn_rfe_cv = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
    "knn_rfe_cv.fit(X_train_rfe, y_train)\n",
    "best_k_rfe = knn_rfe_cv.best_params_['n_neighbors']\n",
    "print(\"Optimal k for RFE features:\", best_k_rfe)\n",
    "knn_model_rfe = KNeighborsClassifier(n_neighbors=best_k_rfe)\n",
    "acc_knn_rfe, cm_knn_rfe = evaluate_model(knn_model_rfe, X_train_rfe, y_train, X_val_rfe, y_val)\n",
    "\n",
    "# Random Forest and SVC on RFE features\n",
    "rf_rfe = RandomForestClassifier(random_state=42)\n",
    "acc_rf_rfe, cm_rf_rfe = evaluate_model(rf_rfe, X_train_rfe, y_train, X_val_rfe, y_val)\n",
    "\n",
    "svc_rfe = SVC(random_state=42)\n",
    "acc_svc_rfe, cm_svc_rfe = evaluate_model(svc_rfe, X_train_rfe, y_train, X_val_rfe, y_val)\n",
    "\n",
    "print(\"\\nAccuracy with RFE-Based Features:\")\n",
    "print(\"KNN:\", acc_knn_rfe)\n",
    "print(\"Random Forest:\", acc_rf_rfe)\n",
    "print(\"SVC:\", acc_svc_rfe)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
